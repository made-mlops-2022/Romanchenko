## Как запускать
```shell
# запускаться из online_inference
docker build . -t my_name/my_tag
docker run -p 80:80 my_name/my_tag
```

Как запустить скрипт, пуляющий запросы (делать после запуска сервиса):
```shell
python3 make_queries.py -n 10
```

Запаблишенный образ

https://hub.docker.com/repository/docker/apollin/online-inference

## Самооценка:
### Основная часть:

1. [+] Оберните inference вашей модели в rest сервис на FastAPI, должен быть endpoint /predict (3 балла)

2. [+] Напишите endpoint /health, который должен возращать 200, если ваша модель готова к работе (такой чек особенно актуален, если делаете доп задание про скачивание из хранилища) (1 балл)

3. [+] Напишите unit тест для /predict (https://fastapi.tiangolo.com/tutorial/testing/, https://flask.palletsprojects.com/en/1.1.x/testing/) (3 балла)

4. [+] Напишите скрипт, который будет делать запросы к вашему сервису (2 балла)

5. [+] Напишите Dockerfile, соберите на его основе образ и запустите локально контейнер (docker build, docker run). Внутри контейнера должен запускаться сервис, написанный в предущем пункте. Закоммитьте его, напишите в README.md корректную команду сборки (4 балла)

6. [+] Опубликуйте образ в https://hub.docker.com/, используя docker push (вам потребуется зарегистрироваться) (2 балла)

7. [+] Опишите в README.md корректные команды docker pull/run, которые должны привести к тому, что локально поднимется на inference ваша модель. Убедитесь, что вы можете протыкать его скриптом из пункта 3 (1 балл)

8. [+] Проведите самооценку - распишите в реквесте какие пункты выполнили и на сколько баллов, укажите общую сумму баллов (1 балл)

### Дополнительная часть:
1. [+] Ваш сервис скачивает модель из S3 или любого другого хранилища при старте, путь для скачивания передается через переменные окружения (+2 доп балла).
   Чтобы выгружать модель + трасформер для данных из S3, надо задать переменную окружения `USE_S3`.
